{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intention of this script is to first isolate key words and phrases common to events in the PostgreSQL database. Then to use these words to search for similar public Facebook events. The attendees for these events are collated into  the specific format requried for Facebook custom audience generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from facebook import GraphAPI\n",
    "import requests\n",
    "import urllib3\n",
    "import numpy as np\n",
    "\n",
    "#has to be updated from Facebook Graph API explorer every so often.\n",
    "token = 'XXXXXX'\n",
    "\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "#contains punctuation library that can be checked against.\n",
    "import string\n",
    "#checking if words in English dictionary\n",
    "#import enchant \n",
    "#for ordering dictionary by values\n",
    "import operator \n",
    "#for counting phrases and sentences\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk import FreqDist\n",
    "import time\n",
    "import pycountry \n",
    "conn = psycopg2.connect(\"dbname=XXXX user=XXXX host=XXXX port=XXXX password=XXXXX\")\n",
    "\n",
    "graph = GraphAPI(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the database and select event information (name, description, location etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"SELECT\n",
    "  events.name,\n",
    "  events.description,\n",
    "  sports.name,\n",
    "  venues.address,\n",
    "  venues.coordinates\n",
    "FROM events\n",
    "  INNER JOIN sports ON events.sport_id = sports.id\n",
    "  INNER JOIN events_venues ON events.id = events_venues.event_id\n",
    "  INNER JOIN venues ON events_venues.venue_id = venues.id\n",
    "\"\"\")\n",
    "out = cur.fetchall()\n",
    "cur.close()\n",
    "\n",
    "cur_two = conn.cursor()\n",
    "cur_two.execute(\"\"\"SELECT\n",
    "  facebook_connections.facebook_id\n",
    "FROM\n",
    "  facebook_connections\n",
    "\"\"\")\n",
    "out_two = cur_two.fetchall()\n",
    "cur_two.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some simple functions for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getKey(item):\n",
    "    return item[1]\n",
    "\n",
    "#function that takes token and returns evenly spaced string (Token in tuple form (\"word\",\"word2\")... so need to remove \"()\" and \",\"\n",
    "def stringify(token):\n",
    "    if len(token) == 1:\n",
    "        result = token[0]\n",
    "    else:\n",
    "        result = \" \"\n",
    "        for i in token:\n",
    "            result += i\n",
    "            result += \" \"\n",
    "    return result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This goes through the database, correctly formats all text associated with the events into a way that allows it to be analysed for key words and phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sport_cohorts = defaultdict(list)\n",
    "for element in out:\n",
    "    sport_cohorts[element[2]].append((element[0],element[1]))\n",
    "sport_cohorts_strings = defaultdict(list)\n",
    "for i in sport_cohorts.keys():\n",
    "    for j in sport_cohorts[i]:\n",
    "            sport_cohorts_strings[i].append(j[0])        \n",
    "            if j[1] != None:\n",
    "                sport_cohorts_strings[i].append(j[1])\n",
    "sport_cohorts_punc = defaultdict(list)\n",
    "for i in sport_cohorts_strings.keys():\n",
    "    for j in sport_cohorts_strings[i]:\n",
    "        for c in string.punctuation:\n",
    "            j = j.replace(c,\"\")\n",
    "        sport_cohorts_punc[i].append(j)\n",
    "full_strings = {}\n",
    "for i in sport_cohorts_punc.keys():\n",
    "    full_strings[i] = \" \"\n",
    "    for j in sport_cohorts_punc[i]:\n",
    "        full_strings[i] += j\n",
    "        full_strings[i] += \" \"\n",
    "big_dictionary = defaultdict(dict)\n",
    "for i in full_strings.keys():\n",
    "    piece = full_strings[i]\n",
    "    tokens = word_tokenize(piece)\n",
    "    counts = dict()\n",
    "    for size in 1, 2, 3, 4, 5:\n",
    "        counts[size] = FreqDist(ngrams(tokens, size))\n",
    "    big_dictionary[i] = counts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look through Facebook Graph API to search for public Facebook events featuring the key words or phrases being searched for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of phrases,  34163\n",
      "10\n",
      "9.33487010002\n",
      "20\n",
      "28.5677599907\n",
      "30\n",
      "32.6485729218\n",
      "40\n",
      "38.400947094\n",
      "50\n",
      "52.9252650738\n",
      "100\n",
      "131.686681986\n",
      "150\n",
      "245.492316961\n",
      "Number of Id's  9713\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def get_information(phrase):\n",
    "    list_IDs = []\n",
    "    link = \"https://graph.facebook.com/search?q={}&type=event&access_token={}\".format(phrase,token)\n",
    "    r = requests.get(link)\n",
    "    result = json.loads(r.text)\n",
    "    if u'error' in result.keys():\n",
    "        print \"BLOCKED AGAIN\"\n",
    "        result = {}\n",
    "    if u'data' in result.keys():\n",
    "        for event in result[u'data']:\n",
    "\n",
    "            if u'id' in event.keys():\n",
    "                if u'place' in event.keys():\n",
    "                    if u'location' in event[u'place']:\n",
    "                        if u'city' in event[u'place'][u'location'].keys():\n",
    "                            city = event[u'place'][u'location'][u'city'].encode('utf8')\n",
    "                        else:\n",
    "                            city = \" \"\n",
    "                        if u'state' in event[u'place'][u'location'].keys():\n",
    "                            state = event[u'place'][u'location'][u'state'].encode('utf8')\n",
    "                        else:\n",
    "                            state = \" \" \n",
    "                        if u'zip' in event[u'place'][u'location'].keys():\n",
    "                            zip_code = event[u'place'][u'location'][u'zip'].encode('utf8')\n",
    "                        else:\n",
    "                            zip_code = \" \"\n",
    "                     \n",
    "                        if u'country' in event[u'place'][u'location'].keys():\n",
    "                        \n",
    "                            mapping = {country.name: country.alpha_2 for country in pycountry.countries}\n",
    "                            country = mapping.get(event[u'place'][u'location'][u'country'])\n",
    "                            if country != None:\n",
    "                                country = country.encode('utf8')\n",
    "                            else: \n",
    "                                country = \" \"\n",
    "                        else:\n",
    "                            country = \" \"\n",
    "                        list_IDs.append((result[u'data'][result[u'data'].index(event)][u'id'],city,state,zip_code,country))\n",
    "                               \n",
    "    list_attendees_IDs = []\n",
    "    for IDs in list_IDs:\n",
    "        ID = IDs[0]\n",
    "        city = IDs[1]\n",
    "        state = IDs[2]\n",
    "        zip_code = IDs[3]\n",
    "        country = IDs[4]\n",
    "        link_two = \"https://graph.facebook.com/{}?fields=attending&access_token={}\".format(ID,token)\n",
    "        p = requests.get(link_two)\n",
    "        result_p = json.loads(p.text)\n",
    "        \n",
    "        if u'attending' in result_p.keys():\n",
    "            for person in result_p[u'attending'][u'data']:\n",
    "                \n",
    "                first_name = person[u'name'].encode('utf8').split(\" \",1)[0]\n",
    "                if len(person[u'name'].encode('utf8').split(\" \",1)) !=1:\n",
    "                    second_name = person[u'name'].encode('utf8').split(\" \",1)[1]\n",
    "                else:\n",
    "                    second_name = \" \"\n",
    "                list_attendees_IDs.append((first_name,second_name,city,state,zip_code,country))\n",
    "            \n",
    "            counter = 1\n",
    "            \n",
    "            while counter <25:\n",
    "                if u'attending' in result_p.keys():\n",
    "                    if u'paging' in result_p[u'attending'].keys():\n",
    "                        if u'next' in result_p[u'attending'][u'paging'].keys():\n",
    "                            q = requests.get(result_p[u'attending'][u'paging'][u'next'])\n",
    "                            result_q = json.loads(q.text)\n",
    "                            if u'data' in result_q.keys():\n",
    "                                for person in result_q[u'data']:\n",
    "                                    first_name = person[u'name'].encode('utf8').split(\" \",1)[0]\n",
    "                                    if len(person[u'name'].encode('utf8').split(\" \",1)) !=1:\n",
    "                                        second_name = person[u'name'].encode('utf8').split(\" \",1)[1]\n",
    "                                    else:\n",
    "                                        second_name = \" \"\n",
    "                                    list_attendees_IDs.append((first_name,second_name,city,state,zip_code,country))\n",
    "                                result_p = result_q\n",
    "                                counter +=1\n",
    "                            else:\n",
    "                                counter = 1000\n",
    "                                result_p = {}\n",
    "                        else:\n",
    "                            counter = 1000\n",
    "                            result_p = {}\n",
    "                    else:\n",
    "                        counter = 1000\n",
    "                        result_p = {}\n",
    "                else:\n",
    "                    if u'paging' in result_p.keys():\n",
    "                        if u'next' in result_p[u'paging'].keys():\n",
    "                            q = requests.get(result_p[u'paging'][u'next'])\n",
    "                            result_q = json.loads(q.text)\n",
    "                            if u'data' in result_q.keys():\n",
    "                                for person in result_q[u'data']:\n",
    "                                    first_name = person[u'name'].encode('utf8').split(\" \",1)[0]\n",
    "                                    if len(person[u'name'].encode('utf8').split(\" \",1)) !=1:\n",
    "                                        second_name = person[u'name'].encode('utf8').split(\" \",1)[1]\n",
    "                                    else:\n",
    "                                        second_name = \" \"\n",
    "                                    list_attendees_IDs.append((first_name,second_name,city,state,zip_code,country))\n",
    "                                \n",
    "                                    result_p = result_q\n",
    "                                    counter +=1\n",
    "                            else:\n",
    "                                counter = 1000\n",
    "                                result_p = {}\n",
    "                        else:\n",
    "                            counter = 1000\n",
    "                            result_p = {}\n",
    "                    else:\n",
    "                        counter = 1000\n",
    "                        result_p = {}\n",
    "    return list_attendees_IDs                    \n",
    "    \n",
    "Sport = \"Football\"\n",
    "Looking_at = 3        #number of words in phrases being searched for \n",
    "\n",
    "print \"Number of phrases, \", len(big_dictionary[Sport][Looking_at].keys())\n",
    "big_list_IDs = []\n",
    "counter = 0\n",
    "start = time.time()\n",
    "for phrases in big_dictionary[Sport][Looking_at].keys()[1301:1500]:\n",
    "    for ID in get_information(phrases):\n",
    "        big_list_IDs.append(ID)\n",
    "    counter +=1\n",
    "    if counter in [10,20,30,40,50,100,150,200,250,300,350,400,450,500,550,600,650,700,750,800,850,900,950]:\n",
    "        print counter\n",
    "        end = time.time()\n",
    "        print (end-start)\n",
    "print \"Number of Id's \",len(big_list_IDs)\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script above generates a list of entires for each person found to be attending one of the public events in question. This script converts this list into a .csv for ease and time saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "d = big_list_IDs\n",
    "with open(\"1500.csv\",\"wb\") as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow([i for i,j,k,v,m,n in d])\n",
    "    writer.writerow([j for i,j,k,v,m,n in d])\n",
    "    writer.writerow([k for i,j,k,v,m,n in d])\n",
    "    writer.writerow([v for i,j,k,v,m,n in d])\n",
    "    writer.writerow([m for i,j,k,v,m,n in d])\n",
    "    writer.writerow([n for i,j,k,v,m,n in d])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script takes all of the .csv files created throughout the above process (above process has to be done several times for different portions of the data set as it is too computationally intensive otherwise).\n",
    "This combines all of this information into one pandas dataframe, formats it into the exact requirements of Facebook custom audience creator. Then converts it all back to a final .csv file for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 ln           ct      zip country\n",
      "fn                                               \n",
      "Paul        Drabick   Pittsburgh    15226      US\n",
      "Maria    Ofi Olvera  San Antonio    78221      US\n",
      "Stephen     Chapman      Calgary  T2E 8P1      CA\n",
      "Connor        Kopko      Boonton    07005      US\n",
      "Clayton     Carroll      Boonton    07005      US\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "list_csv = ['400.csv','600.csv','800.csv','1100.csv','1300.csv','1500.csv']\n",
    "\n",
    "list_final = []\n",
    "\n",
    "df_f = pd.DataFrame()\n",
    "for link in list_csv:\n",
    "    df = pd.read_csv(link)\n",
    "    df_new = pd.concat([df_f,df],axis=1)\n",
    "    df_f = df_new\n",
    "\n",
    "df_transposed = df_f.T\n",
    "\n",
    "\n",
    "#removed state for the moment as it means we lose a lot of data.\n",
    "\n",
    "del df_transposed[2]\n",
    "\n",
    "df_transposed = df_transposed[df_transposed[0] != \" \"]\n",
    "df_transposed = df_transposed[df_transposed[1] != \" \"]\n",
    "#df_transposed = df_transposed[df_transposed[2] != \" \"]\n",
    "df_transposed = df_transposed[df_transposed[3] != \" \"]\n",
    "df_transposed = df_transposed[df_transposed[4] != \" \"]\n",
    "\n",
    "df_transposed.columns = [\"ln\",\"ct\",\"zip\",\"country\"]\n",
    "df_transposed.index.names = [\"fn\"]\n",
    "print df_transposed.head()\n",
    "\n",
    "df_transposed.to_csv(\"name.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
